{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1216\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# test if pyspark is working. Takes a minute to finish.\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 10000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-17-189.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe88e46a630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a SparkSession.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv') \\\n",
    "          .option('header', 'true') \\\n",
    "          .option('delimiter', ',') \\\n",
    "          .option('inferschema', 'true') \\\n",
    "          .load('s3://justinngbucket/Data/movie_lens/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('userId', 'int'),\n",
       " ('movieId', 'int'),\n",
       " ('rating', 'double'),\n",
       " ('timestamp', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|      2|   3.5|1112486027|\n",
      "|     1|     29|   3.5|1112484676|\n",
      "|     1|     32|   3.5|1112484819|\n",
      "|     1|     47|   3.5|1112484727|\n",
      "|     1|     50|   3.5|1112484580|\n",
      "|     1|    112|   3.5|1094785740|\n",
      "|     1|    151|   4.0|1094785734|\n",
      "|     1|    223|   4.0|1112485573|\n",
      "|     1|    253|   4.0|1112484940|\n",
      "|     1|    260|   4.0|1112484826|\n",
      "|     1|    293|   4.0|1112484703|\n",
      "|     1|    296|   4.0|1112484767|\n",
      "|     1|    318|   4.0|1112484798|\n",
      "|     1|    337|   3.5|1094785709|\n",
      "|     1|    367|   3.5|1112485980|\n",
      "|     1|    541|   4.0|1112484603|\n",
      "|     1|    589|   3.5|1112485557|\n",
      "|     1|    593|   3.5|1112484661|\n",
      "|     1|    653|   3.0|1094785691|\n",
      "|     1|    919|   3.5|1094785621|\n",
      "+------+-------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId', 'movieId', 'rating', 'timestamp']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|[Hi, I, heard, ab...|\n",
      "|[I, wish, Java, c...|\n",
      "|[Logistic, regres...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "documentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|              result|\n",
      "+--------------------+--------------------+\n",
      "|[Hi, I, heard, ab...|[0.04915456660091...|\n",
      "|[I, wish, Java, c...|[-0.0332094979073...|\n",
      "|[Logistic, regres...|[0.02375314719974...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.04915456660091877,-0.022600096464157105,0.010528827086091042]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.03320949790733201,-0.015708756932456578,-0.05784856740917478]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.023753147199749948,0.016884601768106224,0.0050010856240987785]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "| (5,[1,3],[1.0,7.0])|\n",
      "|[2.0,0.0,3.0,4.0,...|\n",
      "|[4.0,0.0,0.0,6.0,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7944, 0.2056, 0.0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainedVariance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_4cebb46f4c97c1239d35"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.feature.PCAModel"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|pcaFeatures                                                |\n",
      "+-----------------------------------------------------------+\n",
      "|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------------------+\n",
      "|userId|movieId|rating| timestamp|            features|\n",
      "+------+-------+------+----------+--------------------+\n",
      "|     1|      2|   3.5|1112486027|[1.0,2.0,3.5,1.11...|\n",
      "|     1|     29|   3.5|1112484676|[1.0,29.0,3.5,1.1...|\n",
      "|     1|     32|   3.5|1112484819|[1.0,32.0,3.5,1.1...|\n",
      "|     1|     47|   3.5|1112484727|[1.0,47.0,3.5,1.1...|\n",
      "|     1|     50|   3.5|1112484580|[1.0,50.0,3.5,1.1...|\n",
      "|     1|    112|   3.5|1094785740|[1.0,112.0,3.5,1....|\n",
      "|     1|    151|   4.0|1094785734|[1.0,151.0,4.0,1....|\n",
      "|     1|    223|   4.0|1112485573|[1.0,223.0,4.0,1....|\n",
      "|     1|    253|   4.0|1112484940|[1.0,253.0,4.0,1....|\n",
      "|     1|    260|   4.0|1112484826|[1.0,260.0,4.0,1....|\n",
      "|     1|    293|   4.0|1112484703|[1.0,293.0,4.0,1....|\n",
      "|     1|    296|   4.0|1112484767|[1.0,296.0,4.0,1....|\n",
      "|     1|    318|   4.0|1112484798|[1.0,318.0,4.0,1....|\n",
      "|     1|    337|   3.5|1094785709|[1.0,337.0,3.5,1....|\n",
      "|     1|    367|   3.5|1112485980|[1.0,367.0,3.5,1....|\n",
      "|     1|    541|   4.0|1112484603|[1.0,541.0,4.0,1....|\n",
      "|     1|    589|   3.5|1112485557|[1.0,589.0,3.5,1....|\n",
      "|     1|    593|   3.5|1112484661|[1.0,593.0,3.5,1....|\n",
      "|     1|    653|   3.0|1094785691|[1.0,653.0,3.0,1....|\n",
      "|     1|    919|   3.5|1094785621|[1.0,919.0,3.5,1....|\n",
      "+------+-------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['userId', 'movieId', 'rating', 'timestamp'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(df)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+----------------+\n",
      "|userId|movieId|rating| timestamp|binarized_rating|\n",
      "+------+-------+------+----------+----------------+\n",
      "|     1|      2|   3.5|1112486027|             0.0|\n",
      "|     1|     29|   3.5|1112484676|             0.0|\n",
      "|     1|     32|   3.5|1112484819|             0.0|\n",
      "|     1|     47|   3.5|1112484727|             0.0|\n",
      "|     1|     50|   3.5|1112484580|             0.0|\n",
      "|     1|    112|   3.5|1094785740|             0.0|\n",
      "|     1|    151|   4.0|1094785734|             1.0|\n",
      "|     1|    223|   4.0|1112485573|             1.0|\n",
      "|     1|    253|   4.0|1112484940|             1.0|\n",
      "|     1|    260|   4.0|1112484826|             1.0|\n",
      "|     1|    293|   4.0|1112484703|             1.0|\n",
      "|     1|    296|   4.0|1112484767|             1.0|\n",
      "|     1|    318|   4.0|1112484798|             1.0|\n",
      "|     1|    337|   3.5|1094785709|             0.0|\n",
      "|     1|    367|   3.5|1112485980|             0.0|\n",
      "|     1|    541|   4.0|1112484603|             1.0|\n",
      "|     1|    589|   3.5|1112485557|             0.0|\n",
      "|     1|    593|   3.5|1112484661|             0.0|\n",
      "|     1|    653|   3.0|1094785691|             0.0|\n",
      "|     1|    919|   3.5|1094785621|             0.0|\n",
      "+------+-------+------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=3.5, inputCol=\"rating\", outputCol=\"binarized_rating\")\n",
    "bin_df = binarizer.transform(df)\n",
    "bin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------------------+\n",
      "|userId|movieId|rating| timestamp|            features|\n",
      "+------+-------+------+----------+--------------------+\n",
      "|     1|      2|   3.5|1112486027|[1.0,2.0,3.5,1.11...|\n",
      "|     1|     29|   3.5|1112484676|[1.0,29.0,3.5,1.1...|\n",
      "|     1|     32|   3.5|1112484819|[1.0,32.0,3.5,1.1...|\n",
      "|     1|     47|   3.5|1112484727|[1.0,47.0,3.5,1.1...|\n",
      "|     1|     50|   3.5|1112484580|[1.0,50.0,3.5,1.1...|\n",
      "|     1|    112|   3.5|1094785740|[1.0,112.0,3.5,1....|\n",
      "|     1|    151|   4.0|1094785734|[1.0,151.0,4.0,1....|\n",
      "|     1|    223|   4.0|1112485573|[1.0,223.0,4.0,1....|\n",
      "|     1|    253|   4.0|1112484940|[1.0,253.0,4.0,1....|\n",
      "|     1|    260|   4.0|1112484826|[1.0,260.0,4.0,1....|\n",
      "|     1|    293|   4.0|1112484703|[1.0,293.0,4.0,1....|\n",
      "|     1|    296|   4.0|1112484767|[1.0,296.0,4.0,1....|\n",
      "|     1|    318|   4.0|1112484798|[1.0,318.0,4.0,1....|\n",
      "|     1|    337|   3.5|1094785709|[1.0,337.0,3.5,1....|\n",
      "|     1|    367|   3.5|1112485980|[1.0,367.0,3.5,1....|\n",
      "|     1|    541|   4.0|1112484603|[1.0,541.0,4.0,1....|\n",
      "|     1|    589|   3.5|1112485557|[1.0,589.0,3.5,1....|\n",
      "|     1|    593|   3.5|1112484661|[1.0,593.0,3.5,1....|\n",
      "|     1|    653|   3.0|1094785691|[1.0,653.0,3.0,1....|\n",
      "|     1|    919|   3.5|1094785621|[1.0,919.0,3.5,1....|\n",
      "+------+-------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                     |polyFeatures                                                                                                                           |\n",
      "+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[1.0,2.0,3.5,1.112486027E9]  |[1.0,1.0,2.0,2.0,4.0,3.5,3.5,7.0,12.25,1.112486027E9,1.112486027E9,2.224972054E9,3.8937010945E9,1.23762516027024461E18]                |\n",
      "|[1.0,29.0,3.5,1.112484676E9] |[1.0,1.0,29.0,29.0,841.0,3.5,3.5,101.5,12.25,1.112484676E9,1.112484676E9,3.2262055604E10,3.893696366E9,1.23762215433482496E18]         |\n",
      "|[1.0,32.0,3.5,1.112484819E9] |[1.0,1.0,32.0,32.0,1024.0,3.5,3.5,112.0,12.25,1.112484819E9,1.112484819E9,3.5599514208E10,3.8936968665E9,1.23762247250546278E18]       |\n",
      "|[1.0,47.0,3.5,1.112484727E9] |[1.0,1.0,47.0,47.0,2209.0,3.5,3.5,164.5,12.25,1.112484727E9,1.112484727E9,5.2286782169E10,3.8936965445E9,1.23762226780826445E18]       |\n",
      "|[1.0,50.0,3.5,1.11248458E9]  |[1.0,1.0,50.0,50.0,2500.0,3.5,3.5,175.0,12.25,1.11248458E9,1.11248458E9,5.5624229E10,3.89369603E9,1.23762194073777638E18]              |\n",
      "|[1.0,112.0,3.5,1.09478574E9] |[1.0,1.0,112.0,112.0,12544.0,3.5,3.5,392.0,12.25,1.09478574E9,1.09478574E9,1.2261600288E11,3.83175009E9,1.19855581650734771E18]        |\n",
      "|[1.0,151.0,4.0,1.094785734E9]|[1.0,1.0,151.0,151.0,22801.0,4.0,4.0,604.0,16.0,1.094785734E9,1.094785734E9,1.65312645834E11,4.379142936E9,1.19855580336991872E18]     |\n",
      "|[1.0,223.0,4.0,1.112485573E9]|[1.0,1.0,223.0,223.0,49729.0,4.0,4.0,892.0,16.0,1.112485573E9,1.112485573E9,2.48084282779E11,4.449942292E9,1.23762415013313843E18]     |\n",
      "|[1.0,253.0,4.0,1.11248494E9] |[1.0,1.0,253.0,253.0,64009.0,4.0,4.0,1012.0,16.0,1.11248494E9,1.11248494E9,2.8145868982E11,4.44993976E9,1.23762274172680371E18]        |\n",
      "|[1.0,260.0,4.0,1.112484826E9]|[1.0,1.0,260.0,260.0,67600.0,4.0,4.0,1040.0,16.0,1.112484826E9,1.112484826E9,2.8924605476E11,4.449939304E9,1.23762248808025037E18]     |\n",
      "|[1.0,293.0,4.0,1.112484703E9]|[1.0,1.0,293.0,293.0,85849.0,4.0,4.0,1172.0,16.0,1.112484703E9,1.112484703E9,3.25958017979E11,4.449938812E9,1.23762221440899814E18]    |\n",
      "|[1.0,296.0,4.0,1.112484767E9]|[1.0,1.0,296.0,296.0,87616.0,4.0,4.0,1184.0,16.0,1.112484767E9,1.112484767E9,3.29295491032E11,4.449939068E9,1.23762235680704435E18]    |\n",
      "|[1.0,318.0,4.0,1.112484798E9]|[1.0,1.0,318.0,318.0,101124.0,4.0,4.0,1272.0,16.0,1.112484798E9,1.112484798E9,3.53770165764E11,4.449939192E9,1.2376224257811008E18]    |\n",
      "|[1.0,337.0,3.5,1.094785709E9]|[1.0,1.0,337.0,337.0,113569.0,3.5,3.5,1179.5,12.25,1.094785709E9,1.094785709E9,3.68942783933E11,3.8317499815E9,1.1985557486306327E18]  |\n",
      "|[1.0,367.0,3.5,1.11248598E9] |[1.0,1.0,367.0,367.0,134689.0,3.5,3.5,1284.5,12.25,1.11248598E9,1.11248598E9,4.0828235466E11,3.89370093E9,1.23762505569656038E18]      |\n",
      "|[1.0,541.0,4.0,1.112484603E9]|[1.0,1.0,541.0,541.0,292681.0,4.0,4.0,2164.0,16.0,1.112484603E9,1.112484603E9,6.01854170223E11,4.449938412E9,1.23762199191206758E18]   |\n",
      "|[1.0,589.0,3.5,1.112485557E9]|[1.0,1.0,589.0,589.0,346921.0,3.5,3.5,2061.5,12.25,1.112485557E9,1.112485557E9,6.55253993073E11,3.8936994495E9,1.23762411453360026E18] |\n",
      "|[1.0,593.0,3.5,1.112484661E9]|[1.0,1.0,593.0,593.0,351649.0,3.5,3.5,2075.5,12.25,1.112484661E9,1.112484661E9,6.59703403973E11,3.8936963135E9,1.23762212096028493E18] |\n",
      "|[1.0,653.0,3.0,1.094785691E9]|[1.0,1.0,653.0,653.0,426409.0,3.0,3.0,1959.0,9.0,1.094785691E9,1.094785691E9,7.14895056223E11,3.284357073E9,1.19855570921834752E18]    |\n",
      "|[1.0,919.0,3.5,1.094785621E9]|[1.0,1.0,919.0,919.0,844561.0,3.5,3.5,3216.5,12.25,1.094785621E9,1.094785621E9,1.006107985699E12,3.8317496735E9,1.19855555594835558E18]|\n",
      "+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyExpansion.transform(output).select(['features', 'polyFeatures']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df2)\n",
    "indexed = model.transform(df2)\n",
    "\n",
    "# inputCol must be numeric type.\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)\n",
    "encoded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|     features|\n",
      "+---+--------+-------------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|[0.0,1.0,0.0]|\n",
      "|  1|       b|          2.0|    (2,[],[])|[2.0,0.0,0.0]|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|[1.0,0.0,1.0]|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|[0.0,1.0,0.0]|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|[0.0,1.0,0.0]|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|[1.0,0.0,1.0]|\n",
      "+---+--------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['categoryIndex', 'categoryVec'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "encoded2 = assembler.transform(encoded)\n",
    "encoded2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n",
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| id| v1| v2| v3| v4|\n",
      "+---+---+---+---+---+\n",
      "|  0|1.0|3.0|4.0|3.0|\n",
      "|  0|2.0|4.0|6.0|8.0|\n",
      "+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 1.0, 3.0),\n",
    "    (0, 2.0, 4.0),\n",
    "    (2, 2.0, 5.0)\n",
    "], [\"id\", \"v1\", \"v2\"])\n",
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__ where id = 0\")\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|sum_v1|\n",
      "+---+------+\n",
      "|  0|   3.0|\n",
      "|  2|   2.0|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement=\"SELECT id, sum(v1) as sum_v1 FROM __THIS__ group by id\")\n",
    "sqlTrans.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+-----------+--------------------+\n",
      "| id|   features|              hashes|\n",
      "+---+-----------+--------------------+\n",
      "|  0|  [1.0,1.0]|[[0.0], [0.0], [-...|\n",
      "|  1| [1.0,-1.0]|[[0.0], [-1.0], [...|\n",
      "|  2|[-1.0,-1.0]|[[-1.0], [-1.0], ...|\n",
      "|  3| [-1.0,1.0]|[[-1.0], [0.0], [...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataA = [(0, Vectors.dense([1.0, 1.0]),),\n",
    "         (1, Vectors.dense([1.0, -1.0]),),\n",
    "         (2, Vectors.dense([-1.0, -1.0]),),\n",
    "         (3, Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(4, Vectors.dense([1.0, 0.0]),),\n",
    "         (5, Vectors.dense([-1.0, 0.0]),),\n",
    "         (6, Vectors.dense([0.0, 1.0]),),\n",
    "         (7, Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\n",
      "+---+---+-----------------+\n",
      "|idA|idB|EuclideanDistance|\n",
      "+---+---+-----------------+\n",
      "|  0|  6|              1.0|\n",
      "|  2|  5|              1.0|\n",
      "|  2|  7|              1.0|\n",
      "|  3|  6|              1.0|\n",
      "|  0|  4|              1.0|\n",
      "|  3|  5|              1.0|\n",
      "|  1|  4|              1.0|\n",
      "|  1|  7|              1.0|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"EuclideanDistance\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"EuclideanDistance\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+----------+--------------------+-------+\n",
      "| id|  features|              hashes|distCol|\n",
      "+---+----------+--------------------+-------+\n",
      "|  0| [1.0,1.0]|[[0.0], [0.0], [-...|    1.0|\n",
      "|  1|[1.0,-1.0]|[[0.0], [-1.0], [...|    1.0|\n",
      "+---+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(dfA, key, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
